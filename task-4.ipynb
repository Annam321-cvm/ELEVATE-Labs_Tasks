{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":408,"sourceType":"datasetVersion","datasetId":180}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import pandas library for data manipulation\nimport pandas as pd\n\n# --- Step 1: Loading the Dataset ---\n# Define the path to your dataset.\n# Make sure 'house_prices.csv' is in the same directory as your script,\n# or provide the full path to the file.\nfile_path = '/kaggle/input/breast-cancer-wisconsin-data/data.csv'\n\ntry:\n    data = pd.read_csv(file_path)\n    print(f\"Dataset '{file_path}' loaded successfully!\")\n    print(f\"Shape of the dataset: {data.shape}\")\nexcept FileNotFoundError:\n    print(f\"Error: The file '{file_path}' was not found.\")\n    print(\"Please ensure the file is in the correct directory or provide the full path.\")\n    # It's good practice to exit or handle gracefully if the file isn't found\n    exit()\n\n# Display the first few rows to confirm loading\nprint(\"\\nFirst 5 rows of the dataset:\")\nprint(data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:32:59.092164Z","iopub.execute_input":"2025-06-27T16:32:59.092450Z","iopub.status.idle":"2025-06-27T16:33:01.003208Z","shell.execute_reply.started":"2025-06-27T16:32:59.092427Z","shell.execute_reply":"2025-06-27T16:33:01.002193Z"}},"outputs":[{"name":"stdout","text":"Dataset '/kaggle/input/breast-cancer-wisconsin-data/data.csv' loaded successfully!\nShape of the dataset: (569, 33)\n\nFirst 5 rows of the dataset:\n         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(data.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:33:20.365737Z","iopub.execute_input":"2025-06-27T16:33:20.366038Z","iopub.status.idle":"2025-06-27T16:33:20.373555Z","shell.execute_reply.started":"2025-06-27T16:33:20.366014Z","shell.execute_reply":"2025-06-27T16:33:20.372349Z"}},"outputs":[{"name":"stdout","text":"id                           0\ndiagnosis                    0\nradius_mean                  0\ntexture_mean                 0\nperimeter_mean               0\narea_mean                    0\nsmoothness_mean              0\ncompactness_mean             0\nconcavity_mean               0\nconcave points_mean          0\nsymmetry_mean                0\nfractal_dimension_mean       0\nradius_se                    0\ntexture_se                   0\nperimeter_se                 0\narea_se                      0\nsmoothness_se                0\ncompactness_se               0\nconcavity_se                 0\nconcave points_se            0\nsymmetry_se                  0\nfractal_dimension_se         0\nradius_worst                 0\ntexture_worst                0\nperimeter_worst              0\narea_worst                   0\nsmoothness_worst             0\ncompactness_worst            0\nconcavity_worst              0\nconcave points_worst         0\nsymmetry_worst               0\nfractal_dimension_worst      0\nUnnamed: 32                569\ndtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"\\nDataFrame Information (data.info()):\")\ndata.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:40:10.581355Z","iopub.execute_input":"2025-06-27T16:40:10.581679Z","iopub.status.idle":"2025-06-27T16:40:10.598005Z","shell.execute_reply.started":"2025-06-27T16:40:10.581653Z","shell.execute_reply":"2025-06-27T16:40:10.596870Z"}},"outputs":[{"name":"stdout","text":"\nDataFrame Information (data.info()):\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   diagnosis                569 non-null    int64  \n 1   radius_mean              569 non-null    float64\n 2   texture_mean             569 non-null    float64\n 3   perimeter_mean           569 non-null    float64\n 4   area_mean                569 non-null    float64\n 5   smoothness_mean          569 non-null    float64\n 6   compactness_mean         569 non-null    float64\n 7   concavity_mean           569 non-null    float64\n 8   concave points_mean      569 non-null    float64\n 9   symmetry_mean            569 non-null    float64\n 10  fractal_dimension_mean   569 non-null    float64\n 11  radius_se                569 non-null    float64\n 12  texture_se               569 non-null    float64\n 13  perimeter_se             569 non-null    float64\n 14  area_se                  569 non-null    float64\n 15  smoothness_se            569 non-null    float64\n 16  compactness_se           569 non-null    float64\n 17  concavity_se             569 non-null    float64\n 18  concave points_se        569 non-null    float64\n 19  symmetry_se              569 non-null    float64\n 20  fractal_dimension_se     569 non-null    float64\n 21  radius_worst             569 non-null    float64\n 22  texture_worst            569 non-null    float64\n 23  perimeter_worst          569 non-null    float64\n 24  area_worst               569 non-null    float64\n 25  smoothness_worst         569 non-null    float64\n 26  compactness_worst        569 non-null    float64\n 27  concavity_worst          569 non-null    float64\n 28  concave points_worst     569 non-null    float64\n 29  symmetry_worst           569 non-null    float64\n 30  fractal_dimension_worst  569 non-null    float64\ndtypes: float64(30), int64(1)\nmemory usage: 137.9 KB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 2.2 Display descriptive statistics for numerical columns\n# This shows count, mean, std, min, max, and quartiles for numerical features.\nprint(\"\\nDescriptive Statistics for Numerical Columns (data.describe()):\")\nprint(data.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:40:23.995481Z","iopub.execute_input":"2025-06-27T16:40:23.995826Z","iopub.status.idle":"2025-06-27T16:40:24.057791Z","shell.execute_reply.started":"2025-06-27T16:40:23.995801Z","shell.execute_reply":"2025-06-27T16:40:24.056600Z"}},"outputs":[{"name":"stdout","text":"\nDescriptive Statistics for Numerical Columns (data.describe()):\n        diagnosis  radius_mean  texture_mean  perimeter_mean    area_mean  \\\ncount  569.000000   569.000000    569.000000      569.000000   569.000000   \nmean     0.372583    14.127292     19.289649       91.969033   654.889104   \nstd      0.483918     3.524049      4.301036       24.298981   351.914129   \nmin      0.000000     6.981000      9.710000       43.790000   143.500000   \n25%      0.000000    11.700000     16.170000       75.170000   420.300000   \n50%      0.000000    13.370000     18.840000       86.240000   551.100000   \n75%      1.000000    15.780000     21.800000      104.100000   782.700000   \nmax      1.000000    28.110000     39.280000      188.500000  2501.000000   \n\n       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\ncount       569.000000        569.000000      569.000000           569.000000   \nmean          0.096360          0.104341        0.088799             0.048919   \nstd           0.014064          0.052813        0.079720             0.038803   \nmin           0.052630          0.019380        0.000000             0.000000   \n25%           0.086370          0.064920        0.029560             0.020310   \n50%           0.095870          0.092630        0.061540             0.033500   \n75%           0.105300          0.130400        0.130700             0.074000   \nmax           0.163400          0.345400        0.426800             0.201200   \n\n       symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\ncount     569.000000  ...    569.000000     569.000000       569.000000   \nmean        0.181162  ...     16.269190      25.677223       107.261213   \nstd         0.027414  ...      4.833242       6.146258        33.602542   \nmin         0.106000  ...      7.930000      12.020000        50.410000   \n25%         0.161900  ...     13.010000      21.080000        84.110000   \n50%         0.179200  ...     14.970000      25.410000        97.660000   \n75%         0.195700  ...     18.790000      29.720000       125.400000   \nmax         0.304000  ...     36.040000      49.540000       251.200000   \n\n        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\ncount   569.000000        569.000000         569.000000       569.000000   \nmean    880.583128          0.132369           0.254265         0.272188   \nstd     569.356993          0.022832           0.157336         0.208624   \nmin     185.200000          0.071170           0.027290         0.000000   \n25%     515.300000          0.116600           0.147200         0.114500   \n50%     686.500000          0.131300           0.211900         0.226700   \n75%    1084.000000          0.146000           0.339100         0.382900   \nmax    4254.000000          0.222600           1.058000         1.252000   \n\n       concave points_worst  symmetry_worst  fractal_dimension_worst  \ncount            569.000000      569.000000               569.000000  \nmean               0.114606        0.290076                 0.083946  \nstd                0.065732        0.061867                 0.018061  \nmin                0.000000        0.156500                 0.055040  \n25%                0.064930        0.250400                 0.071460  \n50%                0.099930        0.282200                 0.080040  \n75%                0.161400        0.317900                 0.092080  \nmax                0.291000        0.663800                 0.207500  \n\n[8 rows x 31 columns]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 4. Encode the 'diagnosis' (target) column\nprint(\"\\nEncoding 'diagnosis' column...\")\n# M = Malignant (1), B = Benign (0)\nle = LabelEncoder()\ndata['diagnosis'] = le.fit_transform(data['diagnosis'])\n# Confirm the mapping\nprint(f\"Diagnosis mapping: {list(le.classes_)} -> {list(range(len(le.classes_)))}\")\nprint(\"First 5 rows with encoded diagnosis:\\n\", data['diagnosis'].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:42:25.851959Z","iopub.execute_input":"2025-06-27T16:42:25.852802Z","iopub.status.idle":"2025-06-27T16:42:25.860252Z","shell.execute_reply.started":"2025-06-27T16:42:25.852763Z","shell.execute_reply":"2025-06-27T16:42:25.859245Z"}},"outputs":[{"name":"stdout","text":"\nEncoding 'diagnosis' column...\nDiagnosis mapping: [0, 1] -> [0, 1]\nFirst 5 rows with encoded diagnosis:\n 0    1\n1    1\n2    1\n3    1\n4    1\nName: diagnosis, dtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 5. Separate features (X) and target (y)\nX = data.drop('diagnosis', axis=1)\ny = data['diagnosis']\nprint(f\"\\nShape of X (features): {X.shape}\")\nprint(f\"Shape of y (target): {y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:43:59.142388Z","iopub.execute_input":"2025-06-27T16:43:59.142721Z","iopub.status.idle":"2025-06-27T16:43:59.149860Z","shell.execute_reply.started":"2025-06-27T16:43:59.142695Z","shell.execute_reply":"2025-06-27T16:43:59.148808Z"}},"outputs":[{"name":"stdout","text":"\nShape of X (features): (569, 30)\nShape of y (target): (569,)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 6. Feature Scaling\n# All features are numerical and continuous, so scaling is beneficial.\n# StandardScaler is a good choice as it transforms data to have a mean of 0 and std dev of 1.\nprint(\"\\nApplying StandardScaler to features (X)...\")\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled_data = pd.DataFrame(X_scaled, columns=X.columns) # Convert back to DataFrame for better handling\n\nprint(\"First 5 rows of scaled features (X_scaled_df):\\n\", X_scaled_data.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:44:26.883066Z","iopub.execute_input":"2025-06-27T16:44:26.883418Z","iopub.status.idle":"2025-06-27T16:44:26.903449Z","shell.execute_reply.started":"2025-06-27T16:44:26.883391Z","shell.execute_reply":"2025-06-27T16:44:26.902293Z"}},"outputs":[{"name":"stdout","text":"\nApplying StandardScaler to features (X)...\nFirst 5 rows of scaled features (X_scaled_df):\n    radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n2     1.579888      0.456187        1.566503   1.558884         0.942210   \n3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n\n   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n0          3.283515        2.652874             2.532475       2.217515   \n1         -0.487072       -0.023846             0.548144       0.001392   \n2          1.052926        1.363478             2.037231       0.939685   \n3          3.402909        1.915897             1.451707       2.867383   \n4          0.539340        1.371011             1.428493      -0.009560   \n\n   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n0                2.255747  ...      1.886690      -1.359293         2.303601   \n1               -0.868652  ...      1.805927      -0.369203         1.535126   \n2               -0.398008  ...      1.511870      -0.023974         1.347475   \n3                4.910919  ...     -0.281464       0.133984        -0.249939   \n4               -0.562450  ...      1.298575      -1.466770         1.338539   \n\n   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n0    2.001237          1.307686           2.616665         2.109526   \n1    1.890489         -0.375612          -0.430444        -0.146749   \n2    1.456285          0.527407           1.082932         0.854974   \n3   -0.550021          3.394275           3.893397         1.989588   \n4    1.220724          0.220556          -0.313395         0.613179   \n\n   concave points_worst  symmetry_worst  fractal_dimension_worst  \n0              2.296076        2.750622                 1.937015  \n1              1.087084       -0.243890                 0.281190  \n2              1.955000        1.152255                 0.201391  \n3              2.175786        6.046041                 4.935010  \n4              0.729259       -0.868353                -0.397100  \n\n[5 rows x 30 columns]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 7. Data Splitting (Train-Test Split)\n# Use stratify=y to ensure the proportion of 'M' and 'B' is maintained in both train and test sets.\nprint(\"\\nSplitting data into training and testing sets (80/20 split, stratified)...\")\nX_train, X_test, y_train, y_test = train_test_split(X_scaled_data, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_test: {X_test.shape}\")\nprint(f\"Shape of y_train: {y_train.shape}\")\nprint(f\"Shape of y_test: {y_test.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:44:55.146013Z","iopub.execute_input":"2025-06-27T16:44:55.146363Z","iopub.status.idle":"2025-06-27T16:44:55.158691Z","shell.execute_reply.started":"2025-06-27T16:44:55.146336Z","shell.execute_reply":"2025-06-27T16:44:55.157538Z"}},"outputs":[{"name":"stdout","text":"\nSplitting data into training and testing sets (80/20 split, stratified)...\nShape of X_train: (455, 30)\nShape of X_test: (114, 30)\nShape of y_train: (455,)\nShape of y_test: (114,)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pip install --upgrade scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:51:31.537619Z","iopub.execute_input":"2025-06-27T16:51:31.537958Z","iopub.status.idle":"2025-06-27T16:51:39.380972Z","shell.execute_reply.started":"2025-06-27T16:51:31.537935Z","shell.execute_reply":"2025-06-27T16:51:39.379729Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\nCollecting scikit-learn\n  Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\nRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.0->scikit-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.0->scikit-learn) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.0->scikit-learn) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.0->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.0->scikit-learn) (2024.2.0)\nDownloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.6.1\n    Uninstalling scikit-learn-1.6.1:\n      Successfully uninstalled scikit-learn-1.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:52:49.374127Z","iopub.execute_input":"2025-06-27T16:52:49.374478Z","iopub.status.idle":"2025-06-27T16:52:49.379547Z","shell.execute_reply.started":"2025-06-27T16:52:49.374453Z","shell.execute_reply":"2025-06-27T16:52:49.378645Z"}},"outputs":[{"name":"stdout","text":"1.2.2\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"pip install --upgrade --force-reinstall scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:53:11.483008Z","iopub.execute_input":"2025-06-27T16:53:11.483737Z","iopub.status.idle":"2025-06-27T16:53:51.824524Z","shell.execute_reply.started":"2025-06-27T16:53:11.483698Z","shell.execute_reply":"2025-06-27T16:53:51.823389Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn\n  Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\nCollecting numpy>=1.22.0 (from scikit-learn)\n  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy>=1.8.0 (from scikit-learn)\n  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nUsing cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\nDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n  Attempting uninstall: threadpoolctl\n    Found existing installation: threadpoolctl 3.6.0\n    Uninstalling threadpoolctl-3.6.0:\n      Successfully uninstalled threadpoolctl-3.6.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.0\n    Uninstalling joblib-1.5.0:\n      Successfully uninstalled joblib-1.5.0\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.2\n    Uninstalling scipy-1.15.2:\n      Successfully uninstalled scipy-1.15.2\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.7.0\n    Uninstalling scikit-learn-1.7.0:\n      Successfully uninstalled scikit-learn-1.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.0 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\ncupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.0 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed joblib-1.5.1 numpy-2.3.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\nfrom sklearn.linear_model import LogisticRegression\nprint(\"Logistic Regression imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:54:06.176409Z","iopub.execute_input":"2025-06-27T16:54:06.177150Z","iopub.status.idle":"2025-06-27T16:54:06.204598Z","shell.execute_reply.started":"2025-06-27T16:54:06.177115Z","shell.execute_reply":"2025-06-27T16:54:06.203234Z"}},"outputs":[{"name":"stdout","text":"1.2.2\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3030631780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression imported successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# complete documentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARDRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBayesianRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from ._coordinate_descent import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m from ..base import (\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.11/dist-packages/sklearn/base.py)"],"ename":"ImportError","evalue":"cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.11/dist-packages/sklearn/base.py)","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Re-running previous steps to ensure all variables are defined ---\n# (In a script, you'd have these from the previous execution)\n\n# Load the dataset\ntry:\n    df = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\nexcept FileNotFoundError:\n    print(\"Error: Dataset not found. Please ensure the path is correct.\")\n    exit()\n\n# Drop irrelevant columns\nif 'Unnamed: 32' in df.columns and df['Unnamed: 32'].isnull().all():\n    df = df.drop(columns=['id', 'Unnamed: 32'])\nelse:\n    df = df.drop(columns=['id'])\n\n# Handle Duplicate Rows\ndf.drop_duplicates(inplace=True)\n\n\n# Convert scaled arrays back to DataFrames for consistency (optional but good practice)\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n\n# Apply SMOTE to training data only\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled_df, y_train)\n\nprint(\"--- Data Pre-processing Re-run Complete ---\")\nprint(f\"X_train_resampled shape: {X_train_resampled.shape}\")\nprint(f\"y_train_resampled shape: {y_train_resampled.shape}\")\nprint(f\"X_test_scaled_df shape: {X_test_scaled_df.shape}\")\nprint(f\"y_test shape: {y_test.shape}\\n\")\n\n# --- Logistic Regression Model Fitting ---\n\nprint(\"--- Fitting Logistic Regression Model ---\")\n\n# 1. Initialize the Logistic Regression model\n# solver='liblinear' is a good default for relatively small datasets and works well with L1/L2 regularization.\n# random_state for reproducibility.\nmodel = LogisticRegression(solver='liblinear', random_state=42)\n\n# 2. Train the model using the resampled training data\nprint(\"Training the model on X_train_resampled and y_train_resampled...\")\nmodel.fit(X_train_resampled, y_train_resampled)\n\nprint(\"Model training complete.\\n\")\n\n# --- Model Evaluation ---\n\nprint(\"--- Evaluating the Model ---\")\n\n# Make predictions on the scaled test data\ny_pred = model.predict(X_test_scaled_df)\ny_prob = model.predict_proba(X_test_scaled_df)[:, 1] # Probabilities for the positive class (Malignant)\n\n# 1. Accuracy Score\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# 2. Classification Report (Precision, Recall, F1-Score for each class)\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Benign (0)', 'Malignant (1)']))\n\n# 3. Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(conf_matrix)\n\n# Visualize Confusion Matrix\nplt.figure(figsize=(6, 5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Predicted Benign', 'Predicted Malignant'],\n            yticklabels=['Actual Benign', 'Actual Malignant'])\nplt.title('Confusion Matrix')\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# 4. ROC AUC Score\nroc_auc = roc_auc_score(y_test, y_prob)\nprint(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n\n# 5. ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n\nprint(\"\\n--- Logistic Regression Model Fitting and Evaluation Complete ---\")\nprint(\"Next steps could include hyperparameter tuning, cross-validation, or trying other models.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:52:03.077843Z","iopub.execute_input":"2025-06-27T16:52:03.078136Z","iopub.status.idle":"2025-06-27T16:52:03.111769Z","shell.execute_reply.started":"2025-06-27T16:52:03.078114Z","shell.execute_reply":"2025-06-27T16:52:03.110384Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1571147302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# complete documentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mARDRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBayesianRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from ._coordinate_descent import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m from ..base import (\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.11/dist-packages/sklearn/base.py)"],"ename":"ImportError","evalue":"cannot import name '_fit_context' from 'sklearn.base' (/usr/local/lib/python3.11/dist-packages/sklearn/base.py)","output_type":"error"}],"execution_count":18}]}